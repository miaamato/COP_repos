---
title: A01
author: Mia
date: '2022-03-30'
slug: a01
categories: []
tags: []
---
B1: K-fold cross validation vs Single Split Validation set approach  
>> Both methods are cross validation methods used to avoid overfitting and underfitting the data. A single split validation set randomly splits the dataset into 2 sets, training and test set. The model is trained on the training set and then tested on the test/validation set to see if the model was accurate in predictions. The K-fold method differs in the way that the data can be split any (k) amount of times as long as the folds are about even in size. One of the sets will be the test while the others are trained with the model. This process is repeated k amount of times until each fold is the test set. This is a disadvantage to the k method because of the amount of computing power that would be necessary to run k amount of rounds of training/testing in comparison to the single split validation approach.  

B2: K-fold cross validation vs. LOOCV  
>> K-fold cross validation splits the data into multiple training sets and one test set each resampling of the data. This allows for the model to be trained on multiple sets and have one validation set. The LOOCV method is similar in the way that there is more training sets, however, there is no set for testing- rather it is one observation that is left out as a single test point. This can lead to high variance of the results since you are using almost all the data to guide the target value of a single validation point.   

C: Bootstrapping  
>> Through the bootstrapping method, randomly sampled training sets are produced and used in the model. Because each sample is done with replacement, values can be doubled or omitted, and this promotes less variance in the results but also concludes that the model is solely based on the original sampleâ€™s values. This means that this is a nonparametric method of inferring data so no assumptions can be made about the population that the sample was taken from which could be considered disadvantageous because of this bias. However, if an explicit testing sample is unavailable, bootstrapping may be considered a preferred method especially if looking to quantify the uncertainty of a model.  


# Prep
```{r}
library(ISLR)
library(readxl)
library(boot)
```
## Linear Mod
```{r}
set.seed(1)
# Upload data frame
real.estate <- read.csv("https://raw.githubusercontent.com/miaamato/Data/main/real_estate%20(1).csv")
real.estate
```
```{r}
# variables are renamed 
names(real.estate)[2] <- "X1td"
names(real.estate)[3] <- "X2ha"
names(real.estate)[4] <- "X3dttnms"
names(real.estate)[5] <- "X4nocs"
names(real.estate)[6] <- "X5lat"
names(real.estate)[7] <- "X6long"
names(real.estate)[8] <- "Yhpoua"
colnames(real.estate)
summary(real.estate)
```


```{r}
train <- sample(414,207)
head(train)
attach(real.estate)
```
```{r}
lm.fit <- lm(Yhpoua ~ X1td, data = real.estate, subset = train)
lm.fit
```
```{r}
# residual error
mean((Yhpoua - predict(lm.fit, real.estate))[-train]^2)
```
## Quadratic Function
```{r}
lm.fit.poly <- lm(Yhpoua~poly(X1td,2), data = real.estate, subset = train)
lm.fit.poly
```
```{r}
glm.fit <- glm(Yhpoua~X1td, data = real.estate)
coef(glm.fit)
lm.fitt <- lm(Yhpoua~X1td, data = real.estate)
coef(lm.fitt)
```
```{r}
cv.err <- cv.glm(real.estate, glm.fit)
cv.err$delta
```
```{r}
cv.error <- rep(0,5)
degree <- 1:5
for (d in degree){
  glm.fit <- glm(Yhpoua~poly(X1td, d), data = real.estate)
  cv.error[d] <- cv.glm(real.estate, glm.fit)$delta[1]
}
cv.error
```
```{r}
plot(degree, cv.error, type = 'b')
```

# Kcross x1
```{r}
K = 10
cv.error.1 <- rep(0,5)
degree <- 1:5
for (d in degree){
  glm.fit <- glm(Yhpoua~poly(X1td, d), data = real.estate)
  cv.error.1[d] <- cv.glm(real.estate, glm.fit, K = K)$delta[1]
}
cv.error.1
```
```{r}
plot(degree, cv.error, type = "b")
lines(degree, cv.error.1, type = "b", col = "red")
```
# Kcross x2
```{r}
K = 10
cv.error.2 <- rep(0, 5)
degree <- 1:5
for (d in degree){
  glm.fit <- glm(Yhpoua~poly(X2ha, d), data = real.estate)
  cv.error.2[d] <- cv.glm(real.estate, glm.fit, K = K)$delta[1]
}
cv.error.2
```
```{r}
plot(degree, cv.error, type = "b")
plot(degree, cv.error.2, type = "b", col = "red")
```
# Kcross x3
```{r}
K = 10
cv.error.3 <- rep(0, 5)
degree <- 1:5
for (d in degree){
  glm.fit <- glm(Yhpoua~poly(X3dttnms, d), data = real.estate)
  cv.error.3[d] <- cv.glm(real.estate, glm.fit, K = K)$delta[1]
}
cv.error.3
```
```{r}
plot(degree, cv.error, type = "b")
plot(degree, cv.error.3, type = "b", col = "red")
```

Kcross x4
```{r}
K = 10
cv.error.4 <- rep(0, 5)
degree <- 1:5
for (d in degree){
  glm.fit <- glm(Yhpoua~poly(X4nocs, d), data = real.estate)
  cv.error.4[d] <- cv.glm(real.estate, glm.fit, K = K)$delta[1]
}
cv.error.4
```
```{r}

plot(degree, cv.error, type = "b")
plot(cv.error.4, type = "b", col = "red")
```
# Kcross x5
```{r}
K = 10
cv.error.5 <- rep(0, 5)
degree <- 1:5
for (d in degree){
  glm.fit <- glm(Yhpoua~poly(X5lat, d), data = real.estate)
  cv.error.5[d] <- cv.glm(real.estate, glm.fit, K = K)$delta[1]
}
cv.error.5
```
```{r}
plot(degree, cv.error,type = "b")
plot(degree, cv.error.5, type = "b", col = "red")
# After cv.error.1, the graphs stopped showing the red line of the kcross (when using lines()) to the original cv.error so I've been plotting them separately
```

# Kcross x6
```{r}
K = 10
cv.error.6 <- rep(0, 5)
degree <- 1:5
for (d in degree){
  glm.fit <- glm(Yhpoua~poly(X6long, d), data = real.estate)
  cv.error.6[d] <- cv.glm(real.estate, glm.fit, K = K)$delta[1]
}
cv.error.6
```
```{r}
plot(degree, cv.error,type = "b")
plot(degree, cv.error.6, type = "b", col = "red")
```
# Bootstrap x1
```{r}
## Estimating Accuracy of a Linear Model
boot.fn <- function(data, index){
return(coef(lm(Yhpoua~X1td, data=data,subset=index)))
}
boot.fn(real.estate, 1:414)
```
```{r}
set.seed(1)
boot.fn(real.estate, sample(414, 414, replace = T))
```
```{r}
boot.out <- boot(real.estate, boot.fn, 1000)
plot(boot.out)
```
# Bootstrap x2
```{r}
boot.fn <- function(data, index){
return(coef(lm(Yhpoua~X2ha, data= real.estate,subset=index)))
}
boot.fn(real.estate, 1:414)
set.seed(1)
boot.fn(df, sample(414, 414, replace=T))
boot.out2 <- boot(real.estate, boot.fn, 1000)
```
```{r}
plot(boot.out2)
```
# Bootstrap x3
```{r}
boot.fn <- function(data, index){
return(coef(lm(Yhpoua~X3dttnms, data=real.estate,subset=index)))
}
boot.fn(real.estate, 1:414)
set.seed(1)
boot.fn(df, sample(414, 414, replace=T))
boot.out3 <- boot(real.estate, boot.fn, 1000)
```
```{r}
plot(boot.out3)
```
# Bootstrap x4
```{r}
boot.fn <- function(data, index){
return(coef(lm(Yhpoua~X4nocs, data=real.estate,subset=index)))
}
boot.fn(real.estate, 1:414)
set.seed(1)
boot.fn(df, sample(414, 414, replace=T))
boot.out4 <- boot(real.estate, boot.fn, 1000)
```
```{r}
plot(boot.out4)
```
# Bootstrap x5
```{r}
boot.fn <- function(data, index){
return(coef(lm(Yhpoua~X5lat, data=real.estate,subset=index)))
}
boot.fn(real.estate, 1:414)
set.seed(1)
boot.fn(df, sample(414, 414, replace=T))
boot.out5 <- boot(real.estate, boot.fn, 1000)
```
```{r}
plot(boot.out5)
```
# Bootstrap x6
```{r}
boot.fn <- function(data, index){
return(coef(lm(Yhpoua~X6long, data=real.estate,subset=index)))
}
boot.fn(real.estate, 1:414)
set.seed(1)
boot.fn(df, sample(414, 414, replace=T))
boot.out6 <- boot(real.estate, boot.fn, 1000)
```
```{r}
plot(boot.out6)
```

